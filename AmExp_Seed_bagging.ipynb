{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier,Pool, cv\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((78369, 66), (50226, 65))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"trainF.csv\")\n",
    "test = pd.read_csv(\"testF.csv\")\n",
    "test_id = pd.read_csv(\"test.csv\")\n",
    "test_id = test_id['id']\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    77640\n",
       "1      729\n",
       "Name: redemption_status, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['redemption_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['redemption_status']\n",
    "train.drop('redemption_status', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "     'metric': 'auc',\n",
    "     'objective' : 'binary',\n",
    "     'early_stopping_round':1000,\n",
    "     'bagging_fraction': 0.4,\n",
    "     'colsample_bytree': 0.4,\n",
    "     'feature_fraction': 0.4,\n",
    "     'lambda_l1': 0.0,\n",
    "     'lambda_l2': 0.0,\n",
    "     'learning_rate': 0.001,\n",
    "     'max_depth': 15,\n",
    "     'min_child_samples': 100,\n",
    "     'min_child_weight': 50,\n",
    "     'min_split_gain': 0.0010640577871197205,\n",
    "     'n_estimators': 1520,\n",
    "     'num_leaves': 196,\n",
    "     'subsample': 0.4,\n",
    "     'is_unbalance' :True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'booster' : 'gbtree',\n",
    "    'eta' : 0.1,\n",
    "    'nthread' : 4,\n",
    "    'silent' : True,\n",
    " #   'seed' : 1993,\n",
    "    'scale_pos_weight': 106,\n",
    "    'colsample_bytree': 0.547582701158404,\n",
    " 'gamma': 7.99661203221007,\n",
    " 'learning_rate': 0.036024836896883045,\n",
    " 'max_delta_step': 9.545974437155495,\n",
    " 'max_depth': 4,\n",
    " 'min_child_weight': 3.9108711107457483,\n",
    " 'n_estimators': 1015,\n",
    " 'num_leaves': 100,\n",
    " 'reg_alpha': 6.095828143522551,\n",
    " 'reg_lambda': 2.9017833667247137,\n",
    " 'subsample': 0.9458396927859323\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_XGB(params,trainX, targetX, testX, seed = 0):    \n",
    "    err_xgbX=[]\n",
    "    y_pred_test_xgbX=[]\n",
    "    y_pred_trainCV_xgbX = np.zeros(train.shape[0])\n",
    "    y_pred_train_xgbX = []\n",
    "    from sklearn.model_selection import KFold,StratifiedKFold\n",
    "    fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\n",
    "    i=1\n",
    "    for train_index, test_index in fold.split(trainX,targetX):\n",
    "        x_train, x_test = trainX.iloc[train_index], trainX.iloc[test_index]\n",
    "        y_train, y_test = targetX[train_index], targetX[test_index]\n",
    "        m = XGBClassifier(**params, random_state=1993)\n",
    "        m.fit(x_train,y_train,eval_set=[(x_test, y_test)],eval_metric='auc', early_stopping_rounds=200,verbose=200)\n",
    "        preds=m.predict_proba(x_test)[:,-1]\n",
    "        print(\"err: \",roc_auc_score(y_test,preds))\n",
    "        err_xgbX.append(roc_auc_score(y_test,preds))\n",
    "        p = m.predict_proba(testX)[:,-1]\n",
    "        y_pred_trainCV_xgbX[test_index] = preds \n",
    "        q = m.predict_proba(trainX)[:,-1]\n",
    "        i=i+1\n",
    "        y_pred_train_xgbX.append(q)\n",
    "        y_pred_test_xgbX.append(p)\n",
    "    return np.mean(y_pred_test_xgbX,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LGB(params, trainX, targetX, testX, seed = 0):    \n",
    "    err_lgb=[]\n",
    "    y_pred_test_lgb=[]\n",
    "    y_pred_trainCV_lgb = np.zeros(train.shape[0])\n",
    "    y_pred_train_lgb = []\n",
    "    from sklearn.model_selection import KFold,StratifiedKFold\n",
    "    fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\n",
    "    i=1\n",
    "    for train_index, test_index in fold.split(trainX,targetX):\n",
    "        x_train, x_test = trainX.iloc[train_index], trainX.iloc[test_index]\n",
    "        y_train, y_test = targetX[train_index], targetX[test_index]\n",
    "        m = LGBMClassifier(**params, random_state=1993)\n",
    "        m.fit(x_train,y_train,eval_set=[(x_test, y_test)],eval_metric='auc', early_stopping_rounds=200,verbose=200)\n",
    "        preds=m.predict_proba(x_test)[:,-1]\n",
    "        print(\"err: \",roc_auc_score(y_test,preds))\n",
    "        err_lgb.append(roc_auc_score(y_test,preds))\n",
    "        p = m.predict_proba(testX)[:,-1]\n",
    "        y_pred_trainCV_lgb[test_index] = preds \n",
    "        q = m.predict_proba(trainX)[:,-1]\n",
    "        i=i+1\n",
    "        y_pred_train_lgb.append(q)\n",
    "        y_pred_test_lgb.append(p)\n",
    "    return np.mean(y_pred_test_lgb,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bag(train, test,target, params, seeds, model_type = \"LGB\"):\n",
    "    if model_type not in (\"LGB\", \"XGB\"):\n",
    "        raise ValueError(\"`model_type` must be either `LGB` or `XGB`\")\n",
    "    preds = np.zeros(test.shape[0])\n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(\"#\" * 18)\n",
    "        print(f\"RUN - {i+1} , SEED - {seed}\")\n",
    "        print(\"#\" * 18)\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        os.environ[\"PYTHONHASHSEED\"]=str(seed)\n",
    "        if model_type == 'LGB':\n",
    "            params.update({\n",
    "                \"feature_fraction_seed\" : seed,\n",
    "                \"bagging_fraction_seed\" : seed\n",
    "            })\n",
    "            preds_ = run_LGB(params, train, target, test, seed=seeds[i])\n",
    "        else:\n",
    "            params.update({\"seed\" : seed})\n",
    "            preds_ = run_XGB(params, train, target, test, seed=seeds[i])\n",
    "        preds += preds\n",
    "    return preds / len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_seed = [2019, 2031, 90, 192, 83123, 5601]\n",
    "xgb_seed  = [2119, 1031, 190, 13192, 23123, 5603]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "RUN - 1 , SEED - 2019\n",
      "##################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumarabh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[200]\tvalid_0's auc: 0.930758\n",
      "[400]\tvalid_0's auc: 0.933549\n",
      "[600]\tvalid_0's auc: 0.933994\n",
      "[800]\tvalid_0's auc: 0.934185\n",
      "[1000]\tvalid_0's auc: 0.934062\n",
      "[1200]\tvalid_0's auc: 0.933868\n",
      "[1400]\tvalid_0's auc: 0.933679\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[859]\tvalid_0's auc: 0.934222\n",
      "err:  0.9342215652855116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumarabh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[200]\tvalid_0's auc: 0.942513\n",
      "[400]\tvalid_0's auc: 0.945359\n",
      "[600]\tvalid_0's auc: 0.94627\n",
      "[800]\tvalid_0's auc: 0.946931\n",
      "[1000]\tvalid_0's auc: 0.947289\n",
      "[1200]\tvalid_0's auc: 0.947454\n",
      "[1400]\tvalid_0's auc: 0.947552\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1519]\tvalid_0's auc: 0.947631\n",
      "err:  0.9476310579915732\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "p1 = do_bag(train, test, target, lgb_params, lgb_seed)\n",
    "p2 = do_bag(train, test, target, xgb_params, xgb_seed, model_type = \"XGB\")\n",
    "final_preds = 0.6 * p1 + 0.4 * p2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(columns = ['id','redemption_status'])\n",
    "submit['id'] = test_id\n",
    "submit['redemption_status'] = final_preds\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(\"bagged.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
